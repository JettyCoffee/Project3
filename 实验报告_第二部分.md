## 三、实验设置

### 3.1 损失函数与优化器

**修改意见：需要粘贴一部分的python代码**

​	损失函数的选择直接影响模型的训练效果和最终性能。本实验采用交叉熵损失作为基础损失函数，并引入标签平滑（Label Smoothing）技术进行改进。传统的交叉熵损失对于错误分类的样本会给予较大的惩罚，但这种hard target的方式可能导致模型过度自信，降低泛化能力。本实验设置标签平滑系数ε=0.1，这个参数在防止过拟合和保持分类性能之间取得了良好的平衡。

​	在优化器的选择上，实验采用了随机梯度下降优化器，并配置动量因子为0.9，能够有效加速收敛并减少训练过程中的震荡。

### 3.2 学习率调度

​	本实验采用余弦退火学习率调度（Cosine Annealing）策略，按照余弦函数的形式从初始值平滑地衰减到接近零的值。对初始学习率设置为0.1 。相比于阶梯式学习率调度（Step Decay）和多步学习率调度（MultiStep Decay），余弦退火不需要人工指定学习率下降的具体时间点，这减少了超参数调优的工作量。实验结果表明，余弦退火学习率调度在CIFAR-10分类任务中能够获得更高的最终准确率和更稳定的训练过程。

### 3.3 正则化

​	为了防止模型过拟合并提升泛化能力，实验综合运用了多种正则化技术。首先是前文提到的数据增强，包括随机裁剪、随机翻转和Cutout，这些技术通过扩充训练数据的多样性，使模型能够学习到更加鲁棒的特征表示。其次是批归一化，该技术已经内嵌在ResNet18的架构中，通过标准化每一层的输入分布，不仅加速了训练，还起到了正则化的作用。

​	权重衰减（Weight Decay）是另一种重要的正则化手段，通过在损失函数中加入L2正则化项，惩罚过大的权重值，促使模型学习到更简单、更平滑的决策边界。本实验设置权重衰减系数为5×10⁻⁴，这个值经过多次实验调整，既能够有效防止过拟合，又不会过度限制模型的表达能力。

### 3.4 早停

​	本实验设置的早停容忍度（patience）为15个epoch，最小改善量（min_delta）为0.001。这意味着如果验证集准确率连续15个epoch没有提升超过 0.1% 训练将自动终止。

​	同时实验还实现了模型检查点保存机制。在每个epoch结束后，如果当前模型在验证集上的准确率超过了历史最佳值，系统会自动保存模型权重。这确保了即使训练过程中出现意外中断，也能够恢复最优的模型状态。所有的检查点文件都包含了完整的模型参数、优化器状态和训练历史，为后续的模型分析和继续训练提供了便利。

## 四、实验结果与分析

**修改意见：这里首先需要并列的对比几个模型的结性能，不用过多篇幅，然后对效果最好的wide resnet进行详细分析，分析需要用上所有的图片，然后包含训练过程、最终性能、各类别性能、误分类样本**

