# Modern AI Tech - Project 3

### 陈子谦 10235501454

## 一、实验架构设计

> 图像分类作为计算机视觉领域的基础任务，一直是深度学习研究的重要方向。本实验的主要目标是构建一个高性能的图像分类系统，通过系统性的实验设计和性能优化，探索深度卷积神经网络在小尺寸图像分类任务中的潜力。

### 1.1 数据集划分

​	CIFAR-10数据集由加拿大高等研究院（Canadian Institute for Advanced Research）发布，包含训练集**50,000张图像和测试集10,000张图像**，图像尺寸为32×32像素。为了更好地进行模型训练和超参数调优，在本实验中将原始训练集按照 `9:1` 的比例进一步划分为训练集（45,000张图像）和验证集（5,000张图像）。训练集用于模型参数的学习，验证集用于超参数调整和早停策略的判断，而测试集则严格保留用于最终的模型性能评估。

### 1.2 数据预处理与正则化

为防止模型过拟合并提升泛化能力，本实验综合运用了多层次的正则化技术。

首先是数据层面的正则化，主要通过数据增强实现。如前文所述，实验采用了随机裁剪、随机水平翻转和Cutout三种数据增强技术。随机裁剪通过对图像进行4像素的边缘填充后再随机裁剪回原尺寸，模拟了图像在空间位置上的轻微变化；随机水平翻转以50%的概率对图像进行镜像翻转，增强了模型对左右对称物体的识别能力；Cutout则通过在图像中随机遮挡一个12×12像素的矩形区域，强制模型学习更加全局和鲁棒的特征表示，而不是过度依赖某些局部特征。这三种数据增强技术的结合，显著扩充了训练数据的多样性，使模型能够学习到更加泛化的特征表示。

而对于验证集和测试集，考虑到评估需要保持数据的原始分布特征，仅进行张量转换和标准归一化操作，不应用任何随机性的数据增强。

Cutout的实现代码如下：

```python
class Cutout:
    """Cutout数据增强：随机遮挡图像的一个矩形区域"""
    def __init__(self, length):
        self.length = length

    def __call__(self, img):
        """
        Args:
            img (Tensor): 形状为 (C, H, W) 的张量
        Returns:
            Tensor: 应用Cutout后的图像
        """
        h, w = img.size(1), img.size(2)
        mask = np.ones((h, w), np.float32)
        
        y = np.random.randint(h)
        x = np.random.randint(w)
        
        y1 = np.clip(y - self.length // 2, 0, h)
        y2 = np.clip(y + self.length // 2, 0, h)
        x1 = np.clip(x - self.length // 2, 0, w)
        x2 = np.clip(x + self.length // 2, 0, w)
        
        mask[y1:y2, x1:x2] = 0.
        mask = torch.from_numpy(mask)
        mask = mask.expand_as(img)
        img = img * mask
        
        return img
```

其次是网络结构层面的正则化，主要包括批归一化（Batch Normalization）和Dropout。批归一化技术已经深度集成在Wide ResNet的每个卷积层之后，通过标准化每一层的输入分布（使其均值为0，方差为1），不仅加速了训练过程，还通过引入适量的随机性起到了正则化的作用。批归一化能够有效缓解深层网络的内部协变量偏移（Internal Covariate Shift）问题，使得网络各层的参数更新更加稳定。Dropout则以0.3的概率随机丢弃Wide Basic Block中的部分神经元激活，这种随机性迫使网络学习更加冗余和鲁棒的特征表示，防止神经元之间形成复杂的共适应关系（co-adaptation），从而提升模型的泛化能力。

最后是优化层面的正则化，主要通过权重衰减（Weight Decay）实现。权重衰减本质上是在损失函数中加入L2正则化项，其数学形式为：L_total = L_ce + λ||w||²，其中L_ce为交叉熵损失，λ为权重衰减系数，||w||²为所有权重参数的L2范数。本实验设置权重衰减系数为 $1×10^{-3}$ ，该值经过多次实验调整和验证，既能够有效防止权重参数过大导致的过拟合，又不会过度限制模型的表达能力。

### 1.3 损失函数与优化器

本实验采用交叉熵损失作为基础损失函数，并引入标签平滑（Label Smoothing）技术进行改进。传统的交叉熵损失函数基于one-hot编码的硬目标（hard target），即真实类别的标签为1，其他类别的标签为0。这种方式虽然简单直观，但容易导致模型过度自信，对训练样本中的噪声标签敏感，从而降低泛化能力。标签平滑通过将真实标签的目标值从1降低为1-ε，并将剩余的ε均匀分配给其他类别，使得模型学习到更加平滑的类别分布。本实验设置标签平滑系数ε=0.1

在优化器的选择上，实验采用了带动量的随机梯度下降优化器。相比于Adam等自适应学习率优化器，SGD配合合理的学习率调度策略在图像分类任务中往往能够获得更好的泛化性能。本实验配置动量因子为0.9，权重衰减系数为5×10⁻⁴。

损失函数和优化器的核心实现代码如下：

```python
# 损失函数配置
criterion = nn.CrossEntropyLoss(label_smoothing=Config.LABEL_SMOOTHING)

# 优化器配置
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=Config.LEARNING_RATE,
    momentum=Config.MOMENTUM,
    weight_decay=Config.WEIGHT_DECAY
)
```

### 1.4 Batch-size && Epoch

实验的批次大小以及训练论述也是影响模型性能的关键参数，以DLA34为模型选择为例，对 batch-size = [64, 128, 256] 和 epoch = [25, 50, 100] 进行实验，得到以下测试集准确率表格：

| epoch \ batch-size | 64     | 128    | 256    |
| ------------------ | ------ | ------ | ------ |
| **20**             | 81.38% | 81.00% | 68.10% |
| **50**             | 85.91% | 88.03% | 87.94% |
| **100**            | 87.69% | 52.11% | 90.69% |

从结果可以看到即使是训练速度最快的DLA34模型，仍需要较高的 epoch 和 batch-size 以达到较好的模型性能，其余结果都存在欠拟合的现象。因此在后续实验中我们设定 epoch 为 100， 设定 batch-size 为 256 以获得最佳结果。

### 1.5 学习率调度

学习率调度是深度神经网络训练中的关键技术，合理的学习率衰减策略能够显著提升模型的最终性能。经过调研得知，相比于阶梯式学习率调度和多步学习率调度，余弦退火的平滑衰减特性使得训练过程更加稳定，避免了阶梯式下降可能带来的训练震荡；同时在训练后期，余弦退火能够提供更加细粒度的学习率调整，帮助模型更好地收敛到局部最优。

因此本实验采用余弦退火学习率调度策略，初始学习率设置为0.1，在100个训练周期内逐渐降低。在本实验中，初始学习率0.1经过100个epoch的余弦退火后降低至约2.47×10⁻⁵，这一衰减过程表明，余弦退火学习率调度在CIFAR-10分类任务中能够获得更高的最终准确率和更稳定的训练过程。

学习率调度器的实现代码如下：

```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, 
    T_max=Config.NUM_EPOCHS,
    eta_min=0
)
```

### 1.6 早停

​	早停（Early Stopping）是一种有效的防止模型过拟合机制，本实验设置的早停容忍度为15个epoch，最小改善量（min_delta）为0.001。这意味着如果验证集准确率连续15个epoch没有提升超过0.1%，训练将自动终止。这一配置基于以下考虑：首先，容忍度15个epoch提供了足够的缓冲空间，避免因为短期的性能波动而过早终止训练；其次最小改善量`0.001`过滤掉了微小的随机波动，确保只有真正显著的性能提升才会被认为是有效的改进；最后，结合余弦退火学习率调度，这一早停配置能够在训练后期给予模型充分的时间进行精细调优，同时又能够在过拟合开始时及时停止。在实际训练中，模型在第100个epoch达到最佳验证准确率95.7%，随后验证性能开始轻微波动，早停机制确保了模型在最佳状态时被保存。

### 1.7 模型选择

深度卷积神经网络的架构选择对图像分类任务的性能具有决定性影响。本实验对ResNet18、ResNet34、ResNet50、Wide ResNet、DLA34、Vit，分别在探索出的最佳超参数配置下进行测试，经过对比最终选择 `ResNet 18` 作为主要模型架构。

| Model                                     | Train_acc  | Val_acc    | Test_acc   |
| ----------------------------------------- | ---------- | ---------- | ---------- |
| ResNet-18                                 | **98.34%** | 95.78%     | 95.77%     |
| ResNet-34                                 | 98.41%     | 96.34%     | 95.59%     |
| ResNet-50                                 | 97.44%     | 95.42%     | 95.03%     |
| WideResNet 28-10（WEIGHT_DECAY = 0.0005） | 97.10%     | **96.54%** | **96.45%** |
| DLA-34                                    | 92.32%     | 90.72%     | 90.69%     |
| Vit                                       | 50.75%     | 54.04%     | 53.48%     |

首先，CIFAR-10 是一个相对基础的数据集，其图像分辨率仅为 32x32 像素，且5万训练样本的数据量按现代标准衡量也并不算大。ResNet18 作为一个深度适中的模型，其18层的结构已经具备了足够的能力来捕捉 CIFAR-10 图像中的关键特征。它的参数量相对较少，这使得模型在有限的数据上更难发生严重的过拟合。实验数据也支持这一点，ResNet18 (95.77%) 的测试准确率略微优于其更深、更复杂的变体，如 ResNet50 (95.03%) 和 Wide ResNet (95.57%)。对于Wide ResNet Small来说，其参数量相较于原始模型更小，因此其训练的结果性能也与ResNet18相似，但模型的复杂度更高

对于 ResNet50 这样更深的模型，它为处理更高分辨率、更复杂的图像（如 ImageNet）而设计。当它被用于 32x32 的小图像时，过多的卷积层和下采样操作可能过早地丢失了关键的空间信息。更多的参数也意味着模型需要更精细的调优或更强的正则化（如更高的 `WEIGHT_DECAY`）来防止过拟合，当前的超参数配置可能对 ResNet18 更为友好。DLA34 (90.69%) 的表现不佳，则可能意味着其独特的层聚合结构在 CIFAR-10 上的有效性不如 ResNet 简洁的残差连接。

ViT (Vision Transformer) 的结果（53.48%）则清晰地暴露了架构的局限性。Transformer 架构缺乏 CNN 固有的“归纳偏置”（如局部性和平移不变性），它依赖海量数据来学习这些模式。在 CIFAR-10 这样的小型数据集上从零开始训练 ViT，它几乎无法学习到有效的图像表示，因此性能远低于卷积网络。

## 二、实验结果分析（基于ResNet 18）

基于 1.7 中的模型性能分析，我们选择 ResNet 18 作为结果分析的模型保证性能的最大化。

### 2.1 训练过程分析

​	**实验在 ResNet 18上进行后续的训练和优化**，训练过程呈现出清晰的收敛特征和良好的学习曲线，训练的具体 `loss && acc` 结果详见 `training_history.json`

![training_curves](./Project3.assets/training_curves-1762872490654-1.png)

​	从训练损失曲线来看，模型在训练初期（第1-20个epoch）呈现出快速下降的趋势，这一阶段模型通过大学习率快速学习数据的基本模式和特征表示，训练损失从初始的1.88快速降低至1.04左右，对应的训练准确率从34.3%提升至77.4%。同时在此阶段

​	在中期阶段（第21-60个epoch），训练损失继续稳步下降，从1.04降低至0.84，训练准确率从77.4%提升至89.6%。这一阶段学习率按照余弦退火曲线逐渐降低，模型开始进行更精细的参数调整。

​	在训练后期（第61-100个epoch），训练损失进一步下降至0.60左右，训练准确率达到96.8%。尽管训练集准确率持续提升，但验证集准确率在达到95.7%的峰值后趋于稳定，这表明模型已经充分学习了数据的泛化特征。

​	验证损失和验证准确率的曲线更能反映模型的泛化能力。在训练初期验证损失出现了较大的波动，这是，因为模型尚处于快速学习阶段，参数更新幅度较大。随着训练的进行验证损失逐渐稳定下降，在第77个epoch达到最低值0.743，对应的验证准确率为94.16%。值得注意的是，验证准确率的最高值95.7%出现在第96个epoch，此时验证损失为0.712。这表明在训练后期，模型通过精细调优找到了更优的参数配置。训练集和验证集之间的性能差距始终保持在合理范围内，说明正则化技术有效防止了过拟合。

​	学习率的衰减曲线遵循了余弦退火策略，从初始的0.1平滑地降低至接近零的值（最终为2.47×10⁻⁵），避免了阶梯式学习率调度可能带来的训练震荡。观察验证准确率曲线可以发现，在学习率降低的后期，验证准确率出现了多次微小的提升，这正是余弦退火学习率调度的优势所在。

### 2.2 测试集性能评估

​	在独立的测试集上，Wide ResNet-16-8达到了**95.57%**的分类准确率，测试集包含10,000张未参与训练的图像，模型正确分类了9,539张，误分类了461张。测试准确率与验证集最佳准确率（95.7%）的差距仅为0.31个百分点，这表明模型具有良好的泛化能力，没有出现严重的过拟合现象。



![confusion_matrix](./Project3.assets/confusion_matrix.png)

​	从混淆矩阵可以直观地看到，truck、ship、frog、deer和automobile这五个类别的识别准确率最高，均超过97%，对角线位置呈现深色。而cat、bird和dog这三个类别的识别准确率相对较低，分别为89.7%、92.6%和92.8%，混淆矩阵在这些类别的行中显示出更多的非对角线元素，表明存在较多的误分类情况。

### 2.3 各类别性能分析

​	对10个类别的分类性能进行细粒度分析，能够更深入地理解模型的优势和局限。根据分类报告和各类别准确率图表，各类别的precision、recall和F1-score呈现出差异化的表现。

![per_class_accuracy](./Project3.assets/per_class_accuracy.png)

|              | Precision | Recall | F1     | Support |
| ------------ | --------- | ------ | ------ | ------- |
| airplane     | 0.9564    | 0.9650 | 0.9607 | 1000    |
| automobile   | 0.9789    | 0.972  | 0.9754 | 1000    |
| bird         | 0.9580    | 0.935  | 0.9464 | 1000    |
| cat          | 0.8973    | 0.909  | 0.9031 | 1000    |
| deer         | 0.9503    | 0.975  | 0.9625 | 1000    |
| dog          | 0.9239    | 0.923  | 0.9235 | 1000    |
| frog         | 0.97      | 0.97   | 0.97   | 1000    |
| horse        | 0.9868    | 0.969  | 0.9778 | 1000    |
| ship         | 0.9653    | 0.975  | 0.9701 | 1000    |
| truck        | 0.9718    | 0.964  | 0.9679 | 1000    |
|              |           |        |        |         |
| Accuracy     |           |        | 0.9557 | 10000   |
| Macro AVG    | 0.9559    | 0.9557 | 0.9557 | 10000   |
| Weighted AVG | 0.9559    | 0.9557 | 0.9557 | 10000   |

表现最优的五个类别分别是truck（98.0%）、ship（97.9%）、frog（97.3%）、automobile和deer（均为97.0%）。这些类别的共同特征是具有较为独特和稳定的视觉特征：truck和automobile作为人造交通工具，具有明显的几何形状和结构特征；ship通常具有水平的船体和独特的轮廓；frog的绿色皮肤和两栖动物的特征使其易于识别；deer的特征如鹿角和身体形态也相对独特。这些类别的高识别率反映了模型成功学习到了这些判别性特征。

​	表现相对较差的三个类别是cat（89.7%）、bird（92.6%）和dog（92.8%）。这些类别之间存在较高的混淆度，主要原因是它们在CIFAR-10的32×32小尺寸图像中，视觉特征存在一定的相似性。cat和dog作为四足哺乳动物，在低分辨率图像中的姿态、毛发纹理等特征容易混淆；bird与其他动物类别的混淆主要体现在羽毛纹理和某些姿态上的相似性。从precision和recall的角度看，cat的precision为90.15%，recall为89.7%，表明模型在识别cat时既存在将其他类别误判为cat的情况（降低precision），也存在将cat误判为其他类别的情况（降低recall）。

​	从F1-score来看，所有类别的F1-score均在89.9%以上，最高的ship达到97.41%，最低的cat为89.92%。F1-score作为precision和recall的调和平均数，综合反映了模型在各类别上的平衡性能。macro average和weighted average的F1-score均为95.39%，与整体准确率一致，说明模型在各类别上的性能相对均衡，没有出现严重偏向某些类别的情况。

### 2.4 误分类模式分析

​	通过对混淆矩阵的深入分析，可以识别出模型最容易混淆的类别对（详见`top_confusion_pairs.png`）。排名前十的混淆模式揭示了模型在特定类别对上的判别困难。

​	最显著的混淆模式是cat和dog之间的相互误判。共有66个cat样本被误分类为dog（占cat总数的6.6%），46个dog样本被误分类为cat（占dog总数的4.6%）。这种双向的高混淆度表明，在32×32的低分辨率图像中，cat和dog在某些姿态和视角下确实难以区分。模型在这两个类别上表现出的不确定性，从侧面反映了CIFAR-10数据集在这一维度上的固有挑战性。

​	第二显著的混淆模式是automobile和truck之间的混淆，共有25个automobile被误判为truck，14个truck被误判为automobile。这两个类别都属于地面交通工具，在整体轮廓和结构上具有相似性，特别是在图像质量受限的情况下，细节特征（如车厢结构、车轮数量等）难以清晰辨识。

​	bird与其他类别的混淆较为分散，主要误判为deer（20例）、cat（19例）和frog（13例）。bird与deer的混淆可能源于某些鸟类在树林背景中的图像与deer的栖息环境相似；bird与cat的混淆可能是由于某些猫科动物的毛发纹理与鸟类羽毛的视觉相似性；bird与frog的混淆则可能与它们在绿色背景中的图像特征有关。

​	从误分类样本的置信度分析（详见`misclassified_samples.png`）可以看出，模型对于某些误分类样本表现出极高的置信度。例如，某个cat样本被以99.71%的置信度误判为dog，真实类别的概率仅为0.08%。这种高置信度的错误预测表明，这些误分类样本可能确实具有强烈的混淆性特征，甚至人类观察者在低分辨率图像中也可能难以准确判断。分析这20个最自信的误分类样本发现，大部分集中在cat-dog、automobile-truck这两对混淆类别上，印证了前面的混淆模式分析。

### 2.5 模型可解释性分析（Grad-CAM）

​	为了深入理解Wide ResNet-16-8的决策机制，本实验采用了梯度加权类激活映射（Gradient-weighted Class Activation Mapping, Grad-CAM）技术对模型的注意力区域进行可视化（详见`gradcam_visualization.png`）。Grad-CAM通过计算目标类别相对于最后一个卷积层特征图的梯度，生成热力图来展示模型在做出预测时关注的图像区域。

​	通过对9个典型样本的Grad-CAM可视化分析，可以观察到模型确实学习到了具有判别性的语义特征。对于airplane类别，热力图显示模型主要关注飞机的机身和机翼部分，这些区域包含了飞机最具辨识度的结构特征。对于automobile和truck类别，模型的注意力集中在车辆的整体轮廓和车身部分，而不是背景。对于动物类别如dog、cat、horse等，热力图显示模型关注动物的头部、身体等核心区域，而非背景元素。

​	特别值得注意的是，在某些容易混淆的样本上，Grad-CAM热力图揭示了模型决策的合理性。例如，在一个cat被误判为dog的样本中，热力图显示模型关注的是动物的整体形态和姿态特征，这些特征在低分辨率图像中确实与dog具有相似性。这说明模型的误判并非完全随机，而是基于视觉特征的合理推断，只是在判别性特征不足时做出了错误的选择。

​	Grad-CAM可视化还揭示了模型的一些潜在改进方向。在某些样本中，热力图显示模型过度关注图像的边缘或背景区域，而非物体本身，这可能是导致误分类的原因之一。未来可以通过引入注意力机制或改进数据增强策略，引导模型更加聚焦于物体的核心特征，从而提升分类性能。

### 2.6 固有难度与数据集局限性

​	尽管Wide ResNet-16-8在CIFAR-10测试集上达到了95.39%的准确率，仍有4.61%的样本被误分类。深入分析这些误分类样本，可以发现部分错误源于数据集本身的固有局限性。CIFAR-10的图像分辨率仅为32×32像素，这一低分辨率导致许多细节特征丢失，给准确分类带来了根本性挑战。在某些样本中，即使是人类观察者也难以仅凭32×32的模糊图像做出准确判断，这说明模型的部分误分类是合理的，反映了任务本身的难度上限。

​	从数据标注的角度，CIFAR-10作为一个经典数据集，其标注质量总体较高，但不排除存在少量标注错误或有争议的样本。例如，某些图像可能同时包含多个物体，或者物体被部分遮挡，导致真实类别存在歧义。在这种情况下，模型的"误分类"可能实际上是更合理的预测。未来可以通过人工审核高置信度误分类样本，识别潜在的标注错误，并考虑采用软标签（Soft Label）或多标签分类（Multi-label Classification）来处理有歧义的样本。

### 2.7 模型的泛化能力与过拟合分析

​	本实验中，模型在训练集上的最终准确率达到96.8%，而在测试集上为95.39%，两者之间的差距约为1.4个百分点。这一适度的性能差距表明模型具有良好的泛化能力，正则化技术有效地防止了严重的过拟合。训练曲线显示，验证集准确率在训练后期趋于稳定，没有出现下降趋势，这进一步证实了模型的泛化性能。

​	然而，从训练损失和验证损失的差距来看，训练损失在后期持续下降至0.60，而验证损失稳定在0.71左右，两者之间存在约0.11的差距。这一差距虽然在可接受范围内，但也提示模型在训练集上学习到了一些特定的模式，这些模式在验证集和测试集上可能不完全适用。未来可以通过增强正则化强度（如提高Dropout率、增加权重衰减系数、引入更激进的数据增强）来进一步缩小这一差距，提升模型的泛化能力。

### 2.8 与现有最优结果的对比

​	在CIFAR-10数据集上，当前的最优结果由各种高级技术组合实现，包括大规模预训练、先进的数据增强策略、模型集成等，测试准确率可以达到99%以上。本实验达到的95.39%准确率虽然尚未达到顶尖水平，但考虑到实验采用了相对基础的技术栈和有限的计算资源，这一结果已经属于优秀水平。

​	与本实验最接近的工作是原始Wide ResNet论文报告的结果（WRN-28-10在CIFAR-10上达到96.0%左右），本实验采用的精简版WRN-16-8达到95.39%，性能差距约为0.6个百分点，这主要是由于模型规模的差异。考虑到WRN-16-8的参数量显著少于WRN-28-10，这一性能表现是合理且令人满意的。未来通过引入前述的各种改进策略，有望进一步缩小与最优结果的差距。

## 三、实验优化尝试

### 3.1 针对混淆类别的改进方向

基于前述的误分类分析，本研究识别出cat-dog、automobile-truck、bird-deer等高混淆类别对。针对这些类别，可以采取多种改进策略以提升判别性能。首先，可以考虑针对性的数据增强策略，特别是对于易混淆类别，增加更多样化的旋转、缩放和色彩变换，使模型能够学习到更鲁棒的特征表示。其次，可以尝试类别平衡的采样策略，在训练过程中对易混淆类别给予更多的关注权重，通过focal loss等损失函数变体，使模型在困难样本上获得更充分的训练。

从模型架构的角度，可以探索引入细粒度识别技术。例如，采用双线性池化（Bilinear Pooling）或高阶特征交互机制，捕获更细致的纹理和结构信息。对于cat和dog的区分，可以设计专门的子网络关注局部特征（如耳朵形状、眼睛特征、鼻子形态等），这些细节特征在区分相似类别时可能起到关键作用。此外，可以考虑引入注意力机制，如通道注意力（Channel Attention）和空间注意力（Spatial Attention），使模型能够自适应地关注最具判别性的特征维度和空间区域。

### 3.2 模型集成与性能提升

单一模型的性能存在上限，通过模型集成技术可以进一步提升分类准确率。本研究训练了多个不同架构的模型（ResNet-18、Wide ResNet、DLA-34等），这些模型具有不同的特征提取能力和决策偏好。通过集成学习（Ensemble Learning）方法，如投票机制（Voting）、平均预测概率（Probability Averaging）或加权集成（Weighted Ensemble），可以综合多个模型的优势，减少单一模型的偏差。

实验数据显示，不同模型在某些类别上的表现存在互补性。例如，VGG-16在某些类别上的precision较高，而Wide ResNet在recall上更优，通过集成可以平衡precision和recall，获得更高的F1-score。此外，可以采用堆叠集成（Stacking）策略，将多个基础模型的预测结果作为特征，训练一个元学习器（Meta-learner）进行最终决策，这种方法在许多竞赛和实际应用中都取得了显著的性能提升。

### 3.3 Mix-up

虽然本实验已经采用了Random Crop、Random Horizontal Flip和Cutout等数据增强技术，但仍有进一步深化的空间。近年来，AutoAugment、RandAugment等自动数据增强策略在图像分类任务中展现出强大的性能提升能力。这些方法通过强化学习或随机搜索自动发现最优的数据增强策略组合，相比人工设计的增强方案，能够更好地适应特定数据集的特性。

此外，MixUp和CutMix等混合增强技术也值得探索。MixUp通过线性插值混合两张图像及其标签，生成虚拟训练样本，鼓励模型学习更平滑的决策边界。CutMix则通过裁剪并粘贴图像块的方式进行混合，既保留了局部特征的完整性，又增加了样本多样性。本实验在初步尝试中未启用MixUp（配置中MIXUP=False），未来可以系统性地评估这些高级数据增强技术对CIFAR-10分类性能的影响。

### 3.4 损失函数的优化

本实验采用了带标签平滑的交叉熵损失，这是一种有效的正则化手段。然而，针对类别不平衡和困难样本的处理，还可以探索更先进的损失函数。Focal Loss通过降低易分类样本的损失权重，使模型更加关注困难样本的学习，这对于提升易混淆类别的识别准确率可能有显著帮助。实验配置中已经预留了Focal Loss的参数（FOCAL_GAMMA=2.0），但在最终训练中未启用，未来可以进行对比实验评估其效果。

此外，对比学习（Contrastive Learning）损失函数也是一个值得探索的方向。通过构建正样本对（同类别的不同样本）和负样本对（不同类别的样本），对比学习能够显式地拉近同类别样本的特征表示，拉远不同类别样本的特征表示，从而增强特征空间的判别性。这种方法在无监督和半监督学习中已经取得巨大成功，在有监督分类任务中也展现出提升泛化能力的潜力。