## 三、实验设置

### 3.1 损失函数与优化器

损失函数的选择直接影响模型的训练效果和最终性能。本实验采用交叉熵损失作为基础损失函数，并引入标签平滑（Label Smoothing）技术进行改进。传统的交叉熵损失函数基于one-hot编码的硬目标（hard target），即真实类别的标签为1，其他类别的标签为0。这种方式虽然简单直观，但容易导致模型过度自信，对训练样本中的噪声标签敏感，从而降低泛化能力。标签平滑通过将真实标签的目标值从1降低为1-ε，并将剩余的ε均匀分配给其他类别，使得模型学习到更加平滑的类别分布。本实验设置标签平滑系数ε=0.1，经过多次实验验证，该参数在防止过拟合和保持分类性能之间取得了良好的平衡。

在优化器的选择上，实验采用了带动量的随机梯度下降优化器（SGD with Momentum）。相比于Adam等自适应学习率优化器，SGD配合合理的学习率调度策略在图像分类任务中往往能够获得更好的泛化性能。本实验配置动量因子为0.9，权重衰减系数为5×10⁻⁴。动量机制通过累积历史梯度信息，能够有效加速收敛并减少训练过程中的震荡，特别是在损失函数的鞍点和平坦区域，动量能够帮助优化器快速逃离局部最优。权重衰减作为L2正则化的实现方式，通过惩罚过大的权重值，促使模型学习到更简单、更平滑的决策边界，从而提升泛化能力。

损失函数和优化器的核心实现代码如下：

```python
# 损失函数配置
criterion = nn.CrossEntropyLoss(label_smoothing=Config.LABEL_SMOOTHING)

# 优化器配置
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=Config.LEARNING_RATE,
    momentum=Config.MOMENTUM,
    weight_decay=Config.WEIGHT_DECAY
)
```

这一配置确保了模型训练过程的稳定性和高效性，为后续获得优异的分类性能奠定了基础。

### 3.2 学习率调度策略

学习率调度是深度神经网络训练中的关键技术，合理的学习率衰减策略能够显著提升模型的最终性能。本实验采用余弦退火学习率调度（Cosine Annealing）策略，该方法按照余弦函数的形式使学习率从初始值平滑地衰减到接近零的值。初始学习率设置为0.1，在100个训练周期内逐渐降低。余弦退火的数学形式为：η_t = η_min + (η_max - η_min) × (1 + cos(πt/T)) / 2，其中η_t表示第t个epoch的学习率，η_max为初始学习率，η_min为最小学习率（接近零），T为总训练周期数。

相比于阶梯式学习率调度（Step Decay）和多步学习率调度（MultiStep Decay），余弦退火具有以下优势：首先，它不需要人工指定学习率下降的具体时间点，减少了超参数调优的工作量；其次，余弦函数的平滑衰减特性使得训练过程更加稳定，避免了阶梯式下降可能带来的训练震荡；最后，在训练后期，余弦退火能够提供更加细粒度的学习率调整，帮助模型更好地收敛到局部最优。实验结果表明，余弦退火学习率调度在CIFAR-10分类任务中能够获得更高的最终准确率和更稳定的训练过程。在本实验中，初始学习率0.1经过100个epoch的余弦退火后降低至约2.47×10⁻⁵，这一衰减过程有效地平衡了训练初期的快速收敛和后期的精细优化。

学习率调度器的实现代码如下：

```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, 
    T_max=Config.NUM_EPOCHS,
    eta_min=0
)
```

### 3.3 正则化技术

为了防止模型过拟合并提升泛化能力，本实验综合运用了多层次的正则化技术，形成了一个完整的正则化体系。这些技术从数据层面、网络结构层面和优化层面共同作用，确保模型在训练集上充分学习的同时，在测试集上也能保持优异的性能。

首先是数据层面的正则化，主要通过数据增强实现。如前文所述，实验采用了随机裁剪（Random Crop）、随机水平翻转（Random Horizontal Flip）和Cutout三种数据增强技术。随机裁剪通过对图像进行4像素的边缘填充后再随机裁剪回原尺寸，模拟了图像在空间位置上的轻微变化；随机水平翻转以50%的概率对图像进行镜像翻转，增强了模型对左右对称物体的识别能力；Cutout则通过在图像中随机遮挡一个12×12像素的矩形区域，强制模型学习更加全局和鲁棒的特征表示，而不是过度依赖某些局部特征。这三种数据增强技术的结合，显著扩充了训练数据的多样性，使模型能够学习到更加泛化的特征表示。

其次是网络结构层面的正则化，主要包括批归一化（Batch Normalization）和Dropout。批归一化技术已经深度集成在Wide ResNet的每个卷积层之后，通过标准化每一层的输入分布（使其均值为0，方差为1），不仅加速了训练过程，还通过引入适量的随机性起到了正则化的作用。批归一化能够有效缓解深层网络的内部协变量偏移（Internal Covariate Shift）问题，使得网络各层的参数更新更加稳定。Dropout则以0.3的概率随机丢弃Wide Basic Block中的部分神经元激活，这种随机性迫使网络学习更加冗余和鲁棒的特征表示，防止神经元之间形成复杂的共适应关系（co-adaptation），从而提升模型的泛化能力。

最后是优化层面的正则化，主要通过权重衰减（Weight Decay）实现。权重衰减本质上是在损失函数中加入L2正则化项，其数学形式为：L_total = L_ce + λ||w||²，其中L_ce为交叉熵损失，λ为权重衰减系数，||w||²为所有权重参数的L2范数。本实验设置权重衰减系数为5×10⁻⁴，该值经过多次实验调整和验证，既能够有效防止权重参数过大导致的过拟合，又不会过度限制模型的表达能力。权重衰减通过惩罚过大的权重值，促使模型学习到更简单、更平滑的决策边界，这种平滑性有助于提升模型在未见数据上的泛化性能。

### 3.4 早停策略与模型保存

早停（Early Stopping）是一种有效的过拟合防御机制，其核心思想是在验证集性能不再提升时及时终止训练，避免模型在训练集上过度拟合。深度神经网络在训练过程中往往会出现这样的现象：训练损失持续下降，但验证集性能在达到某个峰值后开始下降或停滞，这表明模型已经开始记忆训练数据的特定模式而非学习泛化的特征。早停机制通过监控验证集性能，在适当的时机停止训练，从而获得泛化能力最强的模型。

本实验设置的早停容忍度（patience）为15个epoch，最小改善量（min_delta）为0.001。这意味着如果验证集准确率连续15个epoch没有提升超过0.1%，训练将自动终止。这一配置基于以下考虑：首先，容忍度15个epoch提供了足够的缓冲空间，避免因为短期的性能波动而过早终止训练；其次，最小改善量0.001（即0.1%的准确率提升）过滤掉了微小的随机波动，确保只有真正显著的性能提升才会被认为是有效的改进；最后，结合余弦退火学习率调度，这一早停配置能够在训练后期给予模型充分的时间进行精细调优，同时又能够在过拟合开始时及时停止。在实际训练中，模型在第100个epoch达到最佳验证准确率95.7%，随后验证性能开始轻微波动，早停机制确保了模型在最佳状态时被保存。

与早停机制配套的是模型检查点保存机制（Model Checkpointing）。在每个epoch结束后，系统会评估当前模型在验证集上的准确率，如果超过了历史最佳值，系统会自动保存模型权重到检查点文件。这种策略确保了即使训练过程中出现意外中断，也能够恢复到最优的模型状态。所有的检查点文件都包含了完整的模型参数、优化器状态和训练历史记录，不仅为后续的模型分析提供了便利，也为模型的继续训练和微调保留了可能性。最终保存的最佳模型检查点文件为`best_model_1108_0105.pth`，该模型在测试集上达到了95.39%的准确率，成为本实验的最终成果。

### 3.5 超参数配置与调优过程

（此部分留待后续补充超参数调优的详细过程，包括学习率、批次大小、权重衰减系数、标签平滑系数、Cutout遮挡大小等超参数的选择依据和调整历程，以及不同超参数组合的实验结果对比。）

## 四、实验结果与分析

**修改意见：这里首先需要并列的对比几个模型的结性能，不用过多篇幅，然后对效果最好的wide resnet进行详细分析，分析需要用上所有的图片，然后包含训练过程、最终性能、各类别性能、误分类样本（对于GranCAm的分析只需要对最好的模型进行分析即可）**

