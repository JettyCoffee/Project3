# Modern AI Tech - Project 3

### 陈子谦 10235501454

## 一、数据集与预处理

> 图像分类作为计算机视觉领域的基础任务，一直是深度学习研究的重要方向。本实验的主要目标是构建一个高性能的图像分类系统，通过系统性的实验设计和性能优化，探索深度卷积神经网络在小尺寸图像分类任务中的潜力。
>

### 1.1 数据集概述

​	CIFAR-10数据集由加拿大高等研究院（Canadian Institute for Advanced Research）发布，包含训练集**50,000张图像和测试集10,000张图像**，图像尺寸为32×32像素。为了更好地进行模型训练和超参数调优，在本实验中将原始训练集按照 `9:1` 的比例进一步划分为训练集（45,000张图像）和验证集（5,000张图像）。训练集用于模型参数的学习，验证集用于超参数调整和早停策略的判断，而测试集则严格保留用于最终的模型性能评估。

### 1.2 数据预处理与增强

​	数据预处理是深度学习训练流程中的关键环节，合理的预处理策略能够显著提升模型的泛化能力。本实验采用了多层次的数据预处理和增强方案，针对训练集和测试集分别设计了不同的处理流程。

​	对于训练集，实验采用了包含多种数据增强技术的预处理流程。首先，应用 Random Crop 操作，先对32×32的图像进行**4个像素**的边缘填充，然后随机裁剪回32×32的尺寸。这种操作通过模拟图像的轻微平移增强模型对位置变化的鲁棒性。其次，以50%的概率对图像进行水平翻转，这对于提升模型对左右对称物体的识别能力具有重要作用。

​	在数据归一化方面，实验采用了CIFAR-10数据集的标准统计参数。每个通道的均值分别为(0.4914, 0.4822, 0.4465)，标准差为(0.2023, 0.1994, 0.2010)。通过零均值归一化，可以加速模型收敛并提升训练稳定性。此外，实验还引入了Cutout数据增强技术，在图像中随机遮挡一个16×16像素的矩形区域**（在尝试了4x4, 8x8, 16x16等多种方案后，最终选定16x16为效果最好的一个）**

​	而对于验证集和测试集，考虑到评估需要保持数据的原始分布特征，仅进行张量转换和标准归一化操作，不应用任何随机性的数据增强。

### 1.3 数据加载策略

​	实验采用PyTorch框架的DataLoader进行高效的数据加载。batch-size设定为128，在计算效率和模型收敛速度之间取得了良好的平衡。为了充分打乱训练数据并提升模型的泛化能力，训练集在每个epoch开始时都会进行随机打乱（shuffle=True）。同时为了充分利用多核CPU资源加速数据加载，设置了8个工作进程并启用了内存固定（pin_memory=True），显著减少了数据加载的等待时间，提升了GPU的利用率。

​	验证集和测试集的批次大小设置为100，由于评估过程不需要反向传播，较大的批次可以加速推理过程。同时评估时关闭了数据打乱，确保每次评估的结果具有可重复性。

## 二、模型架构选择与设计

### 2.1 模型选择策略

深度卷积神经网络的架构选择对图像分类任务的性能具有决定性影响。本实验系统性地评估了多种主流网络架构在CIFAR-10数据集上的表现，包括ResNet系列、VGG系列、Wide ResNet以及Deep Layer Aggregation（DLA）等模型。通过对比不同架构的特点、参数量、训练效率和分类性能，最终确定了适合本任务的最优模型。

ResNet系列作为深度学习领域的经典架构，通过引入残差连接（Residual Connection）有效解决了深层网络的梯度消失问题。本实验测试了ResNet-18和ResNet-50两种配置。ResNet-18包含18层卷积层，参数量约为11.7百万，适合中等规模的图像分类任务。ResNet-50则拥有50层卷积结构，参数量达到25.6百万，具有更强的表达能力，但同时也带来了更高的计算成本和过拟合风险。针对CIFAR-10这类小尺寸图像数据集，实验对ResNet的第一层进行了特殊适配，将原始的7×7卷积核替换为3×3卷积核，并移除了最大池化层，以避免在32×32的小尺寸图像上过早地丢失空间信息。

VGG-16是另一种经典的卷积神经网络架构，其特点是采用小卷积核（3×3）堆叠的策略构建深层网络。本实验实现的VGG-16变体包含13层卷积层和3层全连接层，并在每个卷积层后添加了批归一化（Batch Normalization）以加速收敛和提升性能。VGG架构的参数量较大，约为15百万个可训练参数，且由于其全连接层的设计，容易在小数据集上出现过拟合现象。

Wide ResNet是对传统ResNet的重要改进，其核心思想是通过增加网络的宽度（即增加每层的通道数）而非深度来提升模型性能。相比于单纯加深网络，增加宽度能够在保持训练效率的同时提升模型的表达能力。本实验实现了两种Wide ResNet配置：标准版WRN-28-10（深度28层，宽度因子10）和精简版WRN-16-8（深度16层，宽度因子8）。Wide ResNet在架构中集成了Dropout正则化和Cutout数据增强，这两种技术的结合显著提升了模型的泛化能力。实验结果表明，精简版WRN-16-8在CIFAR-10上达到了95.39%的测试准确率，在参数量和性能之间取得了最佳平衡。

Deep Layer Aggregation（DLA-34）是一种新型的网络架构，通过层级化的特征聚合机制，使不同层次的特征能够更好地融合。DLA-34采用树形结构连接不同深度的特征层，使网络能够同时捕获低级的纹理信息和高级的语义信息。该架构在目标检测和语义分割任务中表现出色，但在CIFAR-10这类相对简单的分类任务上，其复杂的聚合机制并未带来显著的性能提升，反而增加了训练的计算成本。

### 2.2 最终模型选择

经过系统性的对比实验，本研究最终选择了Wide ResNet-16-8作为主要模型架构。这一选择基于以下几方面的考虑：首先，从性能角度，Wide ResNet在CIFAR-10测试集上达到了95.39%的准确率，显著优于ResNet-18的94.7%和VGG-16的89.55%；其次，从效率角度，相比深度更深的ResNet-50和结构更复杂的DLA-34，Wide ResNet-16-8的训练速度更快，收敛更稳定；最后，从泛化能力角度，Wide ResNet通过增加网络宽度而非深度的策略，有效避免了深层网络容易出现的梯度消失和过拟合问题，在验证集和测试集上都表现出优异的泛化性能。

Wide ResNet-16-8的具体配置为：深度16层，宽度因子8，Dropout率0.3，集成Cutout数据增强（遮挡区域大小12×12像素）。该模型的参数量约为11百万，相比标准的WRN-28-10大幅减少，使其能够在有限的计算资源下进行高效训练。模型的主干网络由三个阶段组成，特征通道数分别为128、256和512，每个阶段包含多个Wide Basic Block，通过残差连接和批归一化确保信息的有效传递。网络的最后通过全局平均池化将特征图压缩为向量，再经过全连接层输出10个类别的预测概率。
